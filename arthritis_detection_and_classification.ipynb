{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installing dependencies\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "This command installs essential libraries using the `pip` package manager, which is necessary for our machine learning project. Here’s what each package does:\n",
        "\n",
        "1.  **`torch`**:\n",
        "    \n",
        "    -   PyTorch, one of the most popular machine learning frameworks, allows us to build and train deep learning models. It provides the necessary modules for handling tensors, model layers, optimizers, and more.\n",
        "2.  **`torchvision`**:\n",
        "    \n",
        "    -   A library that extends PyTorch to work specifically with computer vision tasks. It includes common datasets, model architectures (like ResNet), and image transformation functions that make it easy to preprocess and load images for training and inference.\n",
        "3.  **`matplotlib`**:\n",
        "    \n",
        "    -   A visualization library for Python. Here, it’s useful for plotting data and viewing images, such as showing training loss and accuracy trends or displaying example images with predictions.\n",
        "4.  **`opencv-python`**:\n",
        "    \n",
        "    -   OpenCV (Open Source Computer Vision) is a library focused on real-time computer vision. It provides tools for image processing, such as resizing, augmenting, and manipulating images, making it a useful tool for pre-processing X-ray images in our project."
      ],
      "metadata": {
        "id": "M9ItkbvubHwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision matplotlib opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXZMFtQBeVlG",
        "outputId": "f3fc1a5d-a8d6-4119-879b-0d31959df9af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Importing KaggleHub and Downloading the Dataset\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **`import kagglehub`**:\n",
        "    \n",
        "    -   This imports the `kagglehub` library, which provides a simple interface for downloading datasets directly from Kaggle. With `kagglehub`, we can easily access public datasets hosted on Kaggle without manually navigating the website or dealing with authentication tokens.\n",
        "2.  **`path = kagglehub.dataset_download(\"shashwatwork/knee-osteoarthritis-dataset-with-severity\")`**:\n",
        "    \n",
        "    -   This line downloads the specified dataset directly from Kaggle.\n",
        "    -   `\"shashwatwork/knee-osteoarthritis-dataset-with-severity\"` is the dataset identifier on Kaggle, corresponding to a collection of X-ray images graded for knee osteoarthritis severity.\n",
        "    -   The function `dataset_download` retrieves the dataset and stores it locally. The `path` variable holds the file path to the downloaded dataset, which is useful for loading images into our model later.\n",
        "3.  **`print(\"Path to dataset files:\", path)`**:\n",
        "    \n",
        "    -   This simply prints the path to the dataset folder. It allows you to confirm the location of the dataset files, which can be important for organizing your code and loading data correctly."
      ],
      "metadata": {
        "id": "4qacv2PJifLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"shashwatwork/knee-osteoarthritis-dataset-with-severity\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDuNqW9QbHDm",
        "outputId": "7d24cf04-fbef-4d7c-c213-ce2a370a3bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/shashwatwork/knee-osteoarthritis-dataset-with-severity?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204M/204M [00:02<00:00, 75.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Exploring the Dataset Structure\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **`import os`**:\n",
        "    \n",
        "    -   The `os` module in Python provides functions to interact with the operating system, including file and directory handling.\n",
        "2.  **`for dirpath, dirnames, filenames in os.walk(path):`**:\n",
        "    \n",
        "    -   `os.walk(path)` is a generator that recursively traverses the directory specified by `path`.\n",
        "    -   It returns three components:\n",
        "        -   **`dirpath`**: The current directory path being processed.\n",
        "        -   **`dirnames`**: A list of subdirectories within the current directory.\n",
        "        -   **`filenames`**: A list of files within the current directory (typically, image files in this dataset).\n",
        "    -   This loop goes through each directory and its subdirectories to gather all file names, which in our case are X-ray images labeled by osteoarthritis grade (e.g., `0`, `1`, `2`, `3`, `4` for the KL grading scale).\n",
        "3.  **`print(f\"Directory: {dirpath}, Number of images: {len(filenames)}\")`**:\n",
        "    \n",
        "    -   For each directory in the dataset, this line prints the path of the directory (`dirpath`) and the number of images it contains (`len(filenames)`).\n",
        "    -   This is useful for verifying that the dataset is structured as expected, and it gives an overview of the number of images available in each grade category.\n",
        "\n",
        "This code allows you to confirm that all data is correctly organized and identify the number of images per class (e.g., Grade 0, Grade 1) for training."
      ],
      "metadata": {
        "id": "Gy-agR7DbcjN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKg2OCV7SjXy",
        "outputId": "a606a46b-b370-4575-911d-6425190ae645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1, Number of images: 0\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/test, Number of images: 0\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/test/0, Number of images: 639\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/test/3, Number of images: 223\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/test/4, Number of images: 51\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/test/1, Number of images: 296\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/test/2, Number of images: 447\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/val, Number of images: 0\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/val/0, Number of images: 328\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/val/3, Number of images: 106\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/val/4, Number of images: 27\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/val/1, Number of images: 153\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/val/2, Number of images: 212\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/auto_test, Number of images: 0\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/auto_test/0, Number of images: 604\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/auto_test/3, Number of images: 200\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/auto_test/4, Number of images: 44\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/auto_test/1, Number of images: 275\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/auto_test/2, Number of images: 403\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/train, Number of images: 0\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/train/0, Number of images: 2286\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/train/3, Number of images: 757\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/train/4, Number of images: 173\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/train/1, Number of images: 1046\n",
            "Directory: /root/.cache/kagglehub/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity/versions/1/train/2, Number of images: 1516\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(path):\n",
        "    print(f\"Directory: {dirpath}, Number of images: {len(filenames)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Setting Up Data Transformations and Loaders\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **Imports**:\n",
        "    \n",
        "    -   **`torch`** and **`torchvision.transforms`**: Used for defining transformations to prepare image data for the model.\n",
        "    -   **`ImageFolder`**: A `torchvision` utility that loads images from a directory structured by class folders. It assigns labels based on folder names.\n",
        "    -   **`DataLoader`**: A PyTorch class that enables batch loading, shuffling, and parallel processing of data.\n",
        "2.  **Image Size**:\n",
        "    \n",
        "    -   **`IMG_SIZE = 224`** sets the desired image size for all images, which is commonly used in models like ResNet.\n",
        "3.  **Transformations**:\n",
        "    \n",
        "    -   **`train_transform`**:\n",
        "        -   **`Resize((IMG_SIZE, IMG_SIZE))`**: Resizes all images to the defined `IMG_SIZE` (224x224 pixels).\n",
        "        -   **`RandomHorizontalFlip()`**: Applies random horizontal flipping to augment the dataset.\n",
        "        -   **`RandomRotation(15)`**: Randomly rotates images up to ±15 degrees, adding diversity.\n",
        "        -   **`ToTensor()`**: Converts the image to a PyTorch tensor.\n",
        "        -   **`Normalize(mean, std)`**: Normalizes the image using the specified mean and standard deviation, which aligns with pre-trained models’ expectations.\n",
        "    -   **`val_transform`**:\n",
        "        -   Similar to `train_transform`, but without `RandomHorizontalFlip` or `RandomRotation`, since we don’t want augmentation on validation and test data.\n",
        "4.  **Loading the Dataset**:\n",
        "    \n",
        "    -   **`ImageFolder`**:\n",
        "        -   Loads images from directories (`train`, `val`, and `test` folders) and applies the transformations.\n",
        "        -   **`train_dataset`**: Loads training data with `train_transform`.\n",
        "        -   **`val_dataset`** and **`test_dataset`**: Load validation and test data with `val_transform`.\n",
        "5.  **Creating DataLoaders**:\n",
        "    \n",
        "    -   **`DataLoader`**:\n",
        "        -   Wraps the datasets to enable efficient data handling in batches, which helps in managing memory and speeds up training.\n",
        "        -   **`batch_size=32`**: Specifies 32 images per batch, a typical setting for efficient GPU processing.\n",
        "        -   **`shuffle=True` for `train_loader`**: Ensures that data is shuffled during training to prevent the model from learning any specific ordering.\n",
        "        -   **`shuffle=False` for `val_loader` and `test_loader`**: Ensures data is sequential during validation and testing for consistent evaluation."
      ],
      "metadata": {
        "id": "SAZ7qw7ilFF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Define transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = ImageFolder(root=os.path.join(path, \"train\"), transform=train_transform)\n",
        "val_dataset = ImageFolder(root=os.path.join(path, \"val\"), transform=val_transform)\n",
        "test_dataset = ImageFolder(root=os.path.join(path, \"test\"), transform=val_transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "h27x5nUOcvQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Setting Up the Model with Transfer Learning\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **Imports**:\n",
        "    \n",
        "    -   **`torchvision.models`**: Contains various pre-trained models, such as ResNet, which is useful for transfer learning.\n",
        "    -   **`torch.nn`**: Provides layers and modules for building neural networks in PyTorch.\n",
        "2.  **Loading a Pre-trained Model**:\n",
        "    \n",
        "    -   **`model = models.resnet50(pretrained=True)`**:\n",
        "        -   Loads a ResNet-50 model pre-trained on ImageNet, which contains weights optimized for general image recognition.\n",
        "        -   Using a pre-trained model accelerates the training process and improves performance by leveraging previously learned features.\n",
        "3.  **Freezing Layers**:\n",
        "    \n",
        "    -   **`for param in model.parameters(): param.requires_grad = False`**:\n",
        "        -   Sets all model parameters to `requires_grad=False`, which prevents these layers from being updated during backpropagation.\n",
        "        -   This freezes the layers, allowing us to retain learned features while only training the final layer for our specific classification task.\n",
        "4.  **Replacing the Final Layer (Fully Connected Layer)**:\n",
        "    \n",
        "    -   **`model.fc`** is the last fully connected layer of ResNet-50, originally configured to output 1000 classes for ImageNet.\n",
        "    -   **`model.fc = nn.Sequential(...)`** replaces this layer with a custom classifier:\n",
        "        -   **`nn.Linear(num_features, 128)`**: A fully connected layer that takes the output features from the previous layers (`num_features`) and reduces it to 128 units.\n",
        "        -   **`nn.ReLU()`**: An activation function applied after the linear layer, introducing non-linearity.\n",
        "        -   **`nn.Dropout(0.2)`**: A dropout layer that randomly disables 20% of neurons during training to reduce overfitting.\n",
        "        -   **`nn.Linear(128, 5)`**: The final layer that maps the 128 units to 5 output classes, corresponding to the KL grading scale (0-4).\n",
        "5.  **Moving Model to GPU**:\n",
        "    \n",
        "    -   **`device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`**: Checks if a GPU is available and assigns it to `device`; otherwise, defaults to the CPU.\n",
        "    -   **`model = model.to(device)`**: Moves the model to the selected device, optimizing for either GPU or CPU operations.\n",
        "\n",
        "This approach ensures that the model retains powerful image features learned by ResNet-50 while adapting to our knee osteoarthritis classification task."
      ],
      "metadata": {
        "id": "lA9KKPHnlmX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load pre-trained ResNet-50 model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final layer with a custom classifier\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(128, 5)  # 5 classes for KL grading\n",
        ")\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hthbowKHcxnp",
        "outputId": "03268111-8ec6-452e-db91-707a2512425f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 143MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Setting Up the Loss Function and Optimizer\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **Loss Function (`criterion`)**:\n",
        "    \n",
        "    -   **`criterion = nn.CrossEntropyLoss()`**:\n",
        "        -   **CrossEntropyLoss** is commonly used for multi-class classification problems.\n",
        "        -   This loss function calculates the difference between the predicted class probabilities and the true labels. It’s ideal for this project, where the model must classify images into five KL grades (0–4).\n",
        "        -   The **lower** the cross-entropy loss, the better the model’s predictions align with the true labels.\n",
        "2.  **Optimizer (`optimizer`)**:\n",
        "    \n",
        "    -   **`optimizer = optim.Adam(model.fc.parameters(), lr=0.001)`**:\n",
        "        -   **Adam** (Adaptive Moment Estimation) is a widely used optimizer that combines the benefits of both AdaGrad and RMSProp optimizers. It adapts the learning rate for each parameter based on the first and second moments of the gradients, making it efficient and effective for most deep learning tasks.\n",
        "        -   **Parameters of `model.fc`**: We only pass `model.fc.parameters()` to the optimizer, meaning it will only update the weights of our custom fully connected layer (the classifier). Since all other layers are frozen, this saves computation and focuses the training on the final layer that needs to adapt to the new classification task.\n",
        "        -   **Learning Rate (`lr=0.001`)**: A small learning rate helps in fine-tuning the model with minimal risk of large, destabilizing parameter updates."
      ],
      "metadata": {
        "id": "WVjC-yKPl6d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)  # Only training the final layer parameters\n"
      ],
      "metadata": {
        "id": "7bRYw1LjeKrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training and Validation Loop\n",
        "### [*Skip to Step 7.1 if you want to load a pretrained model instead*](#71-loading-the-model)\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **Epoch Loop**:\n",
        "    \n",
        "    -   **`for epoch in range(num_epochs):`**: Sets up a loop to train the model for 20 epochs, where each epoch includes a full pass over the training dataset.\n",
        "    -   **`model.train()`**: Sets the model to training mode, enabling layers like dropout (important for preventing overfitting) if present.\n",
        "2.  **Training Phase**:\n",
        "    \n",
        "    -   **Data Loading**:\n",
        "        \n",
        "        -   **`for images, labels in train_loader:`**: Iterates over the training dataset in batches.\n",
        "        -   **`images, labels = images.to(device), labels.to(device)`**: Moves the images and labels to the appropriate device (CPU/GPU).\n",
        "    -   **Forward and Backward Propagation**:\n",
        "        \n",
        "        -   **`optimizer.zero_grad()`**: Clears any previously accumulated gradients to avoid unwanted accumulation.\n",
        "        -   **`outputs = model(images)`**: Computes predictions for the current batch of images.\n",
        "        -   **`loss = criterion(outputs, labels)`**: Calculates the loss between predictions and true labels.\n",
        "        -   **`loss.backward()`**: Computes gradients for each parameter based on the loss.\n",
        "        -   **`optimizer.step()`**: Updates the model parameters based on the calculated gradients.\n",
        "    -   **Track Loss**:\n",
        "        \n",
        "        -   **`running_loss += loss.item()`**: Accumulates the loss across all batches in the epoch.\n",
        "    -   **Epoch Loss Display**:\n",
        "        \n",
        "        -   **`print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")`**: Displays the average loss for the epoch.\n",
        "3.  **Validation Phase**:\n",
        "    \n",
        "    -   **Set Model to Evaluation Mode**:\n",
        "        -   **`model.eval()`**: Switches the model to evaluation mode, disabling layers such as dropout.\n",
        "    -   **Validation Loss and Accuracy Calculation**:\n",
        "        -   **`with torch.no_grad():`**: Disables gradient computation for faster evaluation.\n",
        "        -   **Validation Data Loop**:\n",
        "            -   Iterates over `val_loader` to calculate validation loss and accuracy.\n",
        "        -   **Calculate Validation Loss**:\n",
        "            -   **`val_loss += loss.item()`**: Accumulates the loss for each batch to compute the overall validation loss.\n",
        "        -   **Calculate Validation Accuracy**:\n",
        "            -   **`_, preds = torch.max(outputs, 1)`**: Retrieves the predicted class for each image by taking the index of the maximum logit value.\n",
        "            -   **`correct += (preds == labels).sum().item()`**: Counts how many predictions match the true labels, giving the total number of correct predictions.\n",
        "        -   **Averaging Validation Loss**:\n",
        "            -   **`val_loss /= len(val_loader)`**: Computes the average validation loss.\n",
        "        -   **Calculate and Print Validation Accuracy**:\n",
        "            -   **`val_accuracy = correct / len(val_dataset)`**: Calculates accuracy by dividing correct predictions by the total number of samples.\n",
        "            -   **`print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")`**: Displays validation loss and accuracy for each epoch."
      ],
      "metadata": {
        "id": "Za1oy-87mIhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = correct / len(val_dataset)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtwZDnS2eMWM",
        "outputId": "887941c9-fb47-4d49-b15f-21ab8594b241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 1.4000\n",
            "Validation Loss: 1.3643, Validation Accuracy: 41.40%\n",
            "Epoch [2/20], Loss: 1.3337\n",
            "Validation Loss: 1.3044, Validation Accuracy: 42.74%\n",
            "Epoch [3/20], Loss: 1.3139\n",
            "Validation Loss: 1.2825, Validation Accuracy: 43.34%\n",
            "Epoch [4/20], Loss: 1.2959\n",
            "Validation Loss: 1.3126, Validation Accuracy: 42.25%\n",
            "Epoch [5/20], Loss: 1.2873\n",
            "Validation Loss: 1.2655, Validation Accuracy: 42.49%\n",
            "Epoch [6/20], Loss: 1.2868\n",
            "Validation Loss: 1.2687, Validation Accuracy: 41.77%\n",
            "Epoch [7/20], Loss: 1.2854\n",
            "Validation Loss: 1.2678, Validation Accuracy: 43.34%\n",
            "Epoch [8/20], Loss: 1.2728\n",
            "Validation Loss: 1.2673, Validation Accuracy: 44.67%\n",
            "Epoch [9/20], Loss: 1.2664\n",
            "Validation Loss: 1.2629, Validation Accuracy: 44.07%\n",
            "Epoch [10/20], Loss: 1.2756\n",
            "Validation Loss: 1.2900, Validation Accuracy: 43.46%\n",
            "Epoch [11/20], Loss: 1.2737\n",
            "Validation Loss: 1.2544, Validation Accuracy: 43.34%\n",
            "Epoch [12/20], Loss: 1.2567\n",
            "Validation Loss: 1.2531, Validation Accuracy: 46.25%\n",
            "Epoch [13/20], Loss: 1.2717\n",
            "Validation Loss: 1.2569, Validation Accuracy: 44.43%\n",
            "Epoch [14/20], Loss: 1.2694\n",
            "Validation Loss: 1.2433, Validation Accuracy: 44.55%\n",
            "Epoch [15/20], Loss: 1.2570\n",
            "Validation Loss: 1.2268, Validation Accuracy: 46.85%\n",
            "Epoch [16/20], Loss: 1.2575\n",
            "Validation Loss: 1.2503, Validation Accuracy: 44.79%\n",
            "Epoch [17/20], Loss: 1.2492\n",
            "Validation Loss: 1.2315, Validation Accuracy: 45.64%\n",
            "Epoch [18/20], Loss: 1.2476\n",
            "Validation Loss: 1.2220, Validation Accuracy: 47.22%\n",
            "Epoch [19/20], Loss: 1.2559\n",
            "Validation Loss: 1.2257, Validation Accuracy: 46.49%\n",
            "Epoch [20/20], Loss: 1.2577\n",
            "Validation Loss: 1.2338, Validation Accuracy: 47.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"71-loading-the-model\"></a>\n",
        "## 7.1. Loading the Model\n",
        "### [*Skip to step 8 if you trained a model in Step 7*](#8-testing-loop-for-model-evaluation)\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **Initialize Model Architecture**:\n",
        "    \n",
        "    -   **`model = models.resnet50(pretrained=True)`**: This line initializes a new instance of the ResNet-50 model. By using `pretrained=True`, it loads the weights from a model pre-trained on ImageNet, which can help improve performance on your specific task.\n",
        "2.  **Modify the Final Layer**:\n",
        "    \n",
        "    -   **`num_features = model.fc.in_features`**: This retrieves the number of input features for the final fully connected layer of the ResNet model.\n",
        "    -   The following lines replace the original fully connected layer with a custom one designed for your specific classification task (detecting arthritis severity), which outputs 5 classes corresponding to different grades of osteoarthritis:\n",
        "        ```python\n",
        "        model.fc = nn.Sequential(\n",
        "            nn.Linear(num_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 5)\n",
        "        )\n",
        "        ```\n",
        "3.  **Load the Model State Dictionary**:\n",
        "    \n",
        "    -   **`model.load_state_dict(torch.load(\"arthritis_detection_model.pth\"))`**: This line loads the model parameters that were saved earlier. The model structure must match the one used during training; otherwise, you may encounter errors.\n",
        "4.  **Move Model to Device**:\n",
        "    \n",
        "    -   **`model = model.to(device)`**: Moves the model to the specified device (CPU or GPU). This ensures that all computations will be performed on the same device as the model.\n",
        "5.  **Set the Model to Evaluation Mode**:\n",
        "    \n",
        "    -   **`model.eval()`**: This sets the model to evaluation mode, which is crucial when performing inference. In this mode, certain layers (like dropout layers) behave differently, ensuring the model uses its learned weights without randomness.\n",
        "6.  **Confirmation Message**:\n",
        "    \n",
        "    -   **`print(\"Model loaded successfully!\")`**: This line provides feedback to the user, confirming that the model has been loaded into memory and is ready for use."
      ],
      "metadata": {
        "id": "--Snm6Y62nq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ohjB2ULKMfx",
        "outputId": "25570578-19e3-496e-96ed-2f91bbdce888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/MyDrive/ML-ArthritisDetection/arthritis_detection_model_1.pth'\n",
        "\n",
        "# Initialize the model architecture first\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Modify the final layer to match your architecture (same as during training)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(128, 5)  # 5 classes for KL grading\n",
        ")\n",
        "\n",
        "# Load the state_dict into the model\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model = model.to(device)\n",
        "model.eval()  # Set to evaluation mode if using for inference\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "6kJ2n_fp2N-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d7c59e7-ff9b-4b79-bd09-9e477aae7c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-28ec95b8ec92>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"8-testing-loop-for-model-evaluation\"></a>\n",
        "## 8. Testing Loop for Model Evaluation\n",
        "\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **Set Model to Evaluation Mode**:\n",
        "    \n",
        "    -   **`model.eval()`**: Switches the model to evaluation mode, which ensures that layers such as dropout are disabled, so the model behaves consistently.\n",
        "2.  **Initialize Test Loss and Accuracy Counters**:\n",
        "    \n",
        "    -   **`test_loss = 0.0`** and **`correct = 0`**: These variables will hold cumulative test loss and the number of correctly predicted samples.\n",
        "3.  **Disable Gradient Calculation**:\n",
        "    \n",
        "    -   **`with torch.no_grad():`**: Prevents gradient computation, speeding up the evaluation and reducing memory usage since gradients aren’t needed for inference.\n",
        "4.  **Iterate Through the Test Dataset**:\n",
        "    \n",
        "    -   **Data Loading**:\n",
        "        \n",
        "        -   **`for images, labels in test_loader:`**: Iterates over batches in the test dataset.\n",
        "        -   **`images, labels = images.to(device), labels.to(device)`**: Moves the images and labels to the same device as the model.\n",
        "    -   **Forward Pass and Loss Calculation**:\n",
        "        \n",
        "        -   **`outputs = model(images)`**: Generates predictions by passing the images through the model.\n",
        "        -   **`loss = criterion(outputs, labels)`**: Computes the loss for the batch, comparing predictions to true labels.\n",
        "        -   **`test_loss += loss.item()`**: Adds the batch loss to the cumulative test loss.\n",
        "    -   **Calculate Accuracy for the Batch**:\n",
        "        \n",
        "        -   **`_, preds = torch.max(outputs, 1)`**: Takes the class with the highest logit as the prediction for each image in the batch.\n",
        "        -   **`correct += (preds == labels).sum().item()`**: Counts the number of correctly classified images and adds to the total.\n",
        "5.  **Calculate Average Test Loss and Accuracy**:\n",
        "    \n",
        "    -   **Average Test Loss**:\n",
        "        -   **`test_loss /= len(test_loader)`**: Divides the cumulative test loss by the number of test batches to get the average.\n",
        "    -   **Test Accuracy**:\n",
        "        -   **`test_accuracy = correct / len(test_dataset)`**: Calculates accuracy as the proportion of correctly classified images to the total number of test images.\n",
        "6.  **Display Results**:\n",
        "    \n",
        "    -   **`print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")`**: Displays the average test loss and accuracy."
      ],
      "metadata": {
        "id": "intvcVovmVTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "test_accuracy = correct / len(test_dataset)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNRsKxhlgUrO",
        "outputId": "149040ed-e3c4-4069-d3bf-e188a5c4f030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.2533, Test Accuracy: 46.98%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Download and Display Image from URL\n",
        "\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **Import Necessary Libraries**:\n",
        "    \n",
        "    -   **`requests`**: Allows you to download the image from a web URL.\n",
        "    -   **`PIL.Image`** and **`BytesIO`**: Handle image operations and conversions.\n",
        "2.  **Define the Image URL**:\n",
        "    \n",
        "    -   **`image_url`**: URL of the knee X-ray image. Here, it points to an online radiology image.\n",
        "3.  **Download and Open the Image**:\n",
        "    \n",
        "    -   **`response = requests.get(image_url)`**: Sends an HTTP GET request to download the image from the specified URL.\n",
        "    -   **`img = Image.open(BytesIO(response.content))`**: Wraps the image content in a `BytesIO` object (to mimic a file) and opens it with `PIL.Image.open()`.\n",
        "4.  **Convert to RGB Mode**:\n",
        "    \n",
        "    -   **`img = img.convert(\"RGB\")`**: Ensures that the image has three color channels (Red, Green, and Blue), which is required for compatibility with the model input.\n",
        "5.  **Display the Image (Optional)**:\n",
        "    \n",
        "    -   **`img.show()`**: Opens the image in the default image viewer. This step is optional and can be used to verify the image visually."
      ],
      "metadata": {
        "id": "YUtsAn4P2rd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# URL of the random knee X-ray image\n",
        "image_url = \"https://upload.orthobullets.com/topic/12287/images/xray6_bilateral_knee-lg.jpg\"\n",
        "\n",
        "# Download the image\n",
        "response = requests.get(image_url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Convert the image to RGB mode to ensure 3 channels\n",
        "img = img.convert(\"RGB\")\n",
        "\n",
        "img.show()  # Optional: displays the image\n"
      ],
      "metadata": {
        "id": "viqYVaMw2u4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Image Preprocessing for Model Prediction\n",
        "\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **Define the Transformation Pipeline**:\n",
        "    \n",
        "    -   **`Resize((224, 224))`**: Resizes the image to 224x224 pixels, matching the input size expected by the model.\n",
        "    -   **`ToTensor()`**: Converts the image from a PIL format to a PyTorch tensor with pixel values normalized between 0 and 1.\n",
        "    -   **`Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`**: Applies normalization using the same mean and standard deviation values as those used during training. These values standardize each channel and are based on the ImageNet dataset.\n",
        "2.  **Apply Transformations**:\n",
        "    \n",
        "    -   **`img_tensor = transform(img)`**: Applies the transformations defined above to the downloaded image.\n",
        "3.  **Add Batch Dimension**:\n",
        "    \n",
        "    -   **`unsqueeze(0)`**: Adds an extra dimension at position 0, making the image tensor shape `[1, 3, 224, 224]`. This batch dimension is required as the model expects input in batches."
      ],
      "metadata": {
        "id": "EGfoSeAHmyIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the same transformations as during training\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Apply transformations to the image\n",
        "img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n"
      ],
      "metadata": {
        "id": "RG_TaOOT29hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Model Inference and Prediction\n",
        "\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **Set the Model to Evaluation Mode**:\n",
        "    \n",
        "    -   **`model.eval()`**: Puts the model in evaluation mode. This disables certain layers like dropout, which are only relevant during training, ensuring that the model’s predictions are deterministic.\n",
        "2.  **Move the Image Tensor to the Correct Device**:\n",
        "    \n",
        "    -   **`img_tensor = img_tensor.to(device)`**: Transfers the image tensor to the same device (GPU or CPU) that the model is currently using. This is essential for avoiding errors during inference.\n",
        "3.  **Perform Inference**:\n",
        "    \n",
        "    -   **`with torch.no_grad():`**: This context manager disables gradient calculations, which reduces memory consumption and speeds up computations since gradients are not needed for inference.\n",
        "    -   **`output = model(img_tensor)`**: Passes the image tensor through the model to get the raw output scores for each class.\n",
        "4.  **Get the Predicted Class**:\n",
        "    \n",
        "    -   **`_, predicted_class = torch.max(output, 1)`**: Uses `torch.max` to obtain the index of the maximum value in the output tensor along the specified dimension (1, which corresponds to the class scores). The first returned value (underscore `_`) represents the maximum score, while `predicted_class` contains the index of the predicted class.\n",
        "    -   **`predicted_class = predicted_class.item()`**: Converts the tensor containing the predicted class index to a standard Python integer for easier handling."
      ],
      "metadata": {
        "id": "2bgon82hm5mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Move the image to the same device as the model (GPU or CPU)\n",
        "img_tensor = img_tensor.to(device)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    output = model(img_tensor)\n",
        "\n",
        "# Get the predicted class\n",
        "_, predicted_class = torch.max(output, 1)\n",
        "predicted_class = predicted_class.item()\n"
      ],
      "metadata": {
        "id": "kZP-WbjT2_Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Mapping Prediction to Human-Readable Labels\n",
        "\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **Define Class Labels**:\n",
        "    \n",
        "    -   A dictionary named `class_labels` is created to map each predicted class index to its corresponding label:\n",
        "        -   **`0`**: \"Healthy knee (Grade 0)\"\n",
        "        -   **`1`**: \"Doubtful (Grade 1)\"\n",
        "        -   **`2`**: \"Minimal osteoarthritis (Grade 2)\"\n",
        "        -   **`3`**: \"Moderate osteoarthritis (Grade 3)\"\n",
        "        -   **`4`**: \"Severe osteoarthritis (Grade 4)\"\n",
        "2.  **Print the Prediction**:\n",
        "    \n",
        "    -   **`print(f\"Model Prediction: {class_labels[predicted_class]}\")`**: This line retrieves the human-readable label using the predicted class index and formats it into a string for output. The `f-string` syntax makes it easy to include variables in strings."
      ],
      "metadata": {
        "id": "ZOCXlNo5nF9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the prediction to a human-readable label\n",
        "class_labels = {\n",
        "    0: \"Healthy knee (Grade 0)\",\n",
        "    1: \"Doubtful (Grade 1)\",\n",
        "    2: \"Minimal osteoarthritis (Grade 2)\",\n",
        "    3: \"Moderate osteoarthritis (Grade 3)\",\n",
        "    4: \"Severe osteoarthritis (Grade 4)\"\n",
        "}\n",
        "\n",
        "print(f\"Model Prediction: {class_labels[predicted_class]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJmoVUpt3BCY",
        "outputId": "58e7ce57-e73f-47d5-dc1a-c2a5e64a91f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Prediction: Minimal osteoarthritis (Grade 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. (Optional) Saving the Trained Model\n",
        "\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "1.  **Saving the Model**:\n",
        "    -   **`torch.save(model.state_dict(), \"arthritis_detection_model.pth\")`**: This line saves the model's parameters (weights and biases) to a file named `arthritis_detection_model.pth`.\n",
        "        -   **`model.state_dict()`**: Retrieves the state dictionary of the model, which contains all the learnable parameters. This is a common way to save a model in PyTorch, allowing you to later reload the parameters without needing to retrain the model.\n",
        "2.  **Confirmation Message**:\n",
        "    -   **`print(\"Model saved successfully!\")`**: This line provides feedback that the model has been saved, confirming to the user that the operation was successful."
      ],
      "metadata": {
        "id": "MdRR_-sZ2l8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"arthritis_detection_model.pth\")\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDnwDGyk2Ioq",
        "outputId": "9f94ce86-e8ea-4cd7-a0d5-9eb21973077e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ]
    }
  ]
}